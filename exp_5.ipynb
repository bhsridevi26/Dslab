{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/20wh1a6610/DS-Lab/blob/main/exp_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV2tfR_1nw_0",
        "outputId": "52f48895-222f-4554-8c72-5e0db125c0aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['go', 'jurong', 'pot', 'crazi', 'avail', 'bugi', 'great', 'world', 'la', 'buffet', 'ce', 'got', 'amor', 'wat'], ['ok', 'lar', 'joke', 'wif', 'oni']]\n",
            "Word2Vec<vocab=7259, vector_size=500, alpha=0.025>\n",
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    4 3644 2044  654  593 1161   77  335  897\n",
            "  2560 1701   15 3645   78]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     9  292  594  437 1702]]\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n",
            "Epoch 1/3\n",
            "279/279 [==============================] - 36s 124ms/step - loss: 0.4088 - acc: 0.8627 - val_loss: 0.5281 - val_acc: 0.8664\n",
            "Epoch 2/3\n",
            "279/279 [==============================] - 27s 97ms/step - loss: 0.1325 - acc: 0.9578 - val_loss: 0.1521 - val_acc: 0.9722\n",
            "Epoch 3/3\n",
            "279/279 [==============================] - 28s 99ms/step - loss: 0.0330 - acc: 0.9924 - val_loss: 0.0833 - val_acc: 0.9767\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d8948b8bd60>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd '/content/drive/MyDrive/'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Dropout, Conv1D, MaxPool1D, GlobalMaxPool1D, Embedding, Activation\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import PorterStemmer\n",
        "from sklearn import preprocessing\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(sen):\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "    stops = stopwords.words('english')\n",
        "    porter = PorterStemmer()\n",
        "    for word in sentence.split():\n",
        "        if word in stops:\n",
        "            sentence = sentence.replace(word, '')\n",
        "        sentence = sentence.replace(word, porter.stem(word))\n",
        "    return sentence.lower()\n",
        "\n",
        "\n",
        "def gensim_to_keras_embedding(model, train_embeddings=False):\n",
        "\n",
        "    keyed_vectors = model.wv\n",
        "    weights = keyed_vectors.vectors\n",
        "    index_to_key = keyed_vectors.index_to_key\n",
        "\n",
        "    layer = Embedding(\n",
        "        input_dim=weights.shape[0],\n",
        "        output_dim=weights.shape[1],\n",
        "        weights=[weights],\n",
        "        trainable=train_embeddings,\n",
        "    )\n",
        "    return layer\n",
        "\n",
        "df = pd.read_csv('SPAM_Data.csv')\n",
        "df['Message'] = df['Message'].apply(preprocess_text)\n",
        "df.head(10)\n",
        "\n",
        "mes = []\n",
        "for i in df['Message']:\n",
        "    mes.append(i.split())\n",
        "print(mes[:2])\n",
        "\n",
        "word2vec_model = Word2Vec(mes, vector_size=500, window=3, min_count=1, workers=16)\n",
        "print(word2vec_model)\n",
        "\n",
        "token = Tokenizer(7259)\n",
        "token.fit_on_texts(df['Message'])\n",
        "text = token.texts_to_sequences(df['Message'])\n",
        "text = pad_sequences(text, 75)\n",
        "print(text[:2])\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "y = le.fit_transform(df['Category'])\n",
        "y = to_categorical(y)\n",
        "print(y[:10])\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(np.array(text), y, test_size=0.2, stratify=y)\n",
        "keras_model = Sequential()\n",
        "#keras_model.add(word2vec_model.wv.get_keras_embedding(True))\n",
        "keras_model.add(gensim_to_keras_embedding(word2vec_model, True))\n",
        "keras_model.add(Dropout(0.2))\n",
        "keras_model.add(Conv1D(50, 3, activation='relu', padding='same', strides=1))\n",
        "keras_model.add(MaxPool1D())\n",
        "keras_model.add(Dropout(0.2))\n",
        "keras_model.add(Conv1D(100, 3, activation='relu', padding='same', strides=1))\n",
        "keras_model.add(MaxPool1D())\n",
        "keras_model.add(Dropout(0.2))\n",
        "keras_model.add(GlobalMaxPool1D())\n",
        "#keras_model.add(Dropout(0.2))\n",
        "#keras_model.add(Dense(200))\n",
        "keras_model.add(Activation('relu'))\n",
        "keras_model.add(Dropout(0.2))\n",
        "keras_model.add(Dense(2))\n",
        "keras_model.add(Activation('softmax'))\n",
        "keras_model.compile(loss='binary_crossentropy', metrics=['acc'], optimizer='adam')\n",
        "keras_model.fit(x_train, y_train, batch_size=16, epochs=3, validation_data=(x_test, y_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}